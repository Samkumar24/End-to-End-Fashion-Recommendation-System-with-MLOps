{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8c8e2580",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'c:\\\\Users\\\\sam\\\\End-to-End-Fashion-Recommendation-System-with-MLOps\\\\research'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "67ac30de",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0ad8a88a",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir('../')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "eb67c640",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'c:\\\\Users\\\\sam\\\\End-to-End-Fashion-Recommendation-System-with-MLOps'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "14f72338",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b3cd5129",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "import os\n",
    "from pathlib import Path\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d297710a",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass(frozen=True)\n",
    "class Data_validation_config:\n",
    "    raw_data_path:      Path\n",
    "    validated_data:     Path\n",
    "    fallback_data:      Path\n",
    "    log_path:           Path\n",
    "    expected_columns:   list[str]\n",
    "    missing_thresholds: dict[str, float]\n",
    "    volume:             dict[str, int]\n",
    "    valid_aesthetics:   list[str]\n",
    "    range_checks:       dict[str, dict]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c8123225",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.recommendation_system.utils.common import read_yaml , create_dir\n",
    "from src.recommendation_system.logging import logger\n",
    "from src.recommendation_system.constants import CONFIG_PATH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d990e8b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Config_manager:\n",
    "\n",
    "    def __init__(self, config_path = CONFIG_PATH):\n",
    "        self.config_path = read_yaml(config_path)\n",
    "\n",
    "        create_dir([self.config_path.artifacts_root])\n",
    "\n",
    "    \n",
    "    def get_data_validation_config(self) -> Data_validation_config:\n",
    "\n",
    "        config = self.config_path.data_validation\n",
    "\n",
    "        create_dir([config.validated_data , \n",
    "                    config.fallback_data,\n",
    "                    os.path.dirname(config.log_path)])\n",
    "        \n",
    "        data_validation_config = Data_validation_config(\n",
    "        raw_data_path =     config.raw_data_path,\n",
    "        validated_data     = config.validated_data,\n",
    "        fallback_data      = config.fallback_data,\n",
    "        log_path           = config.log_path,\n",
    "        expected_columns   = config.expected_columns,\n",
    "        missing_thresholds = dict(config.missing_thresholds),\n",
    "        volume             = dict(config.volume),\n",
    "        valid_aesthetics   = config.valid_aesthetics,\n",
    "        range_checks       = dict(config.range_checks),\n",
    ")\n",
    "        return data_validation_config\n",
    "\n",
    "\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6e9713f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Data_validation_check:\n",
    "\n",
    "    def __init__(self, config: Data_validation_config):\n",
    "        # store config so all methods can use it\n",
    "        self.config = config\n",
    "\n",
    "    # ── check 1 — schema ──────────────────────────────────\n",
    "    def check_schema(self, df):\n",
    "\n",
    "        # what columns you expect from config.yaml\n",
    "        expected_columns = self.config.expected_columns\n",
    "\n",
    "        # what columns actually arrived in new data\n",
    "        received_columns = list(df.columns)\n",
    "\n",
    "        # columns present in expected but missing in new data\n",
    "        # eg: scraper stopped collecting discount column\n",
    "        missing_columns = [\n",
    "            col for col in expected_columns\n",
    "            if col not in received_columns\n",
    "        ]\n",
    "\n",
    "        # columns present in new data but not expected\n",
    "        # eg: scraper added a new column you dont know about\n",
    "        extra_columns = [\n",
    "            col for col in received_columns\n",
    "            if col not in expected_columns\n",
    "        ]\n",
    "\n",
    "        return missing_columns, extra_columns\n",
    "\n",
    "    # ── check 2 — missing values ──────────────────────────\n",
    "    def check_missing_val(self, df):\n",
    "\n",
    "        errors = []\n",
    "\n",
    "        # loop through each column and its allowed threshold\n",
    "        # eg: rating allows max 45% missing\n",
    "        for col, threshold in self.config.missing_thresholds.items():\n",
    "\n",
    "            missing_pct = df[col].isna().mean()\n",
    "\n",
    "            # if actual missing is above allowed threshold\n",
    "            if missing_pct > threshold:\n",
    "                errors.append(\n",
    "                    f\"{col}: {missing_pct:.1%} missing - exceeds {threshold:.1%}\"\n",
    "                )\n",
    "\n",
    "        # empty list means all columns passed\n",
    "        return errors\n",
    "\n",
    "    # ── check 3 — volume ──────────────────────────────────\n",
    "    def check_volume(self, df):\n",
    "\n",
    "        # actual number of rows that arrived\n",
    "        rows = len(df)\n",
    "\n",
    "        # min and max rows allowed from config.yaml\n",
    "        min_rows = self.config.volume['min_rows']\n",
    "        max_rows = self.config.volume['max_rows']\n",
    "\n",
    "        # 0 rows means scraper completely failed\n",
    "        # website was down or scraper crashed\n",
    "        if rows == 0:\n",
    "            return f\"0 rows - scraper completely failed\"\n",
    "\n",
    "        # too few rows means scraper partially failed\n",
    "        # amazon may have blocked some requests\n",
    "        if rows < min_rows:\n",
    "            return f\"Too few rows: {rows} - minimum is {min_rows}\"\n",
    "\n",
    "        # too many rows means scraper ran twice\n",
    "        # every product duplicated\n",
    "        if rows > max_rows:\n",
    "            return f\"Too many rows: {rows} - maximum is {max_rows}\"\n",
    "\n",
    "        # None means no error - row count is normal\n",
    "        return None\n",
    "\n",
    "    # ── check 4 — categories ──────────────────────────────\n",
    "    def check_categories(self, df):\n",
    "\n",
    "        # your 7 known aesthetics from config.yaml\n",
    "        valid = set(self.config.valid_aesthetics)\n",
    "\n",
    "        # unique aesthetics found in new data\n",
    "        # dropna removes missing values before checking\n",
    "        found = set(df['aesthetic'].dropna().unique())\n",
    "\n",
    "        unseen = found - valid\n",
    "\n",
    "        if unseen:\n",
    "            return f\"Unseen aesthetics: {unseen}\"\n",
    "\n",
    "        # None means all aesthetics are known\n",
    "        return None\n",
    "\n",
    "    # ── check 5 — ranges ──────────────────────────────────\n",
    "    def check_ranges(self, df):\n",
    "\n",
    "        errors = []\n",
    "\n",
    "        # loop through each column and its min/max bounds\n",
    "        for col, bounds in self.config.range_checks.items():\n",
    "\n",
    "            # skip if column not present\n",
    "            # check 1 already caught that\n",
    "            if col not in df.columns:\n",
    "                continue\n",
    "\n",
    "            # ignore missing values when checking range\n",
    "            col_data = df[col].dropna()\n",
    "\n",
    "            # find rows where value is below min OR above max\n",
    "            out_of_range = col_data[\n",
    "                (col_data < bounds['min']) |\n",
    "                (col_data > bounds['max'])\n",
    "            ]\n",
    "\n",
    "            # if any rows found outside range add to errors\n",
    "            if len(out_of_range) > 0:\n",
    "                errors.append(\n",
    "                    f\"{col}: {len(out_of_range)} rows out of range \"\n",
    "                    f\"({bounds['min']} - {bounds['max']})\"\n",
    "                )\n",
    "\n",
    "        # empty list means all columns passed range check\n",
    "        return errors\n",
    "    \n",
    "    def validate_data(self):\n",
    "        try:\n",
    "            logger.info(\"=\" * 40)\n",
    "            logger.info(\"DATA VALIDATION STARTED\")\n",
    "            logger.info(\"=\" * 40)\n",
    "\n",
    "            # load raw data from path in config\n",
    "            # no hardcoding - path comes from config.yaml\n",
    "            df = pd.read_csv(self.config.raw_data_path)\n",
    "            logger.info(f\"Loaded: {len(df)} rows from {self.config.raw_data_path}\")\n",
    "            logger.info(\"-\" * 40)\n",
    "\n",
    "            # ── check 1 ───────────────────────────────────\n",
    "            logger.info(\"CHECK 1 - SCHEMA\")\n",
    "            missing_col, extra_col = self.check_schema(df)\n",
    "\n",
    "            # missing columns = hard failure\n",
    "            # no point running other checks\n",
    "            if missing_col:\n",
    "                logger.error(f\"FAILED - Missing columns: {missing_col}\")\n",
    "                self.save_log(\"FAILED\", len(df), [f\"Missing columns: {missing_col}\"])\n",
    "                return False\n",
    "\n",
    "            # extra columns = just a warning, not a failure\n",
    "            if extra_col:\n",
    "                logger.warning(f\"WARNING - Extra columns: {extra_col}\")\n",
    "\n",
    "            logger.info(\"PASSED - Check 1 Schema\")\n",
    "            logger.info(\"-\" * 40)\n",
    "\n",
    "            # ── check 2 ───────────────────────────────────\n",
    "            logger.info(\"CHECK 2 - MISSING VALUES\")\n",
    "            missing_errors = self.check_missing_val(df)\n",
    "\n",
    "            # any column above threshold = failure\n",
    "            if missing_errors:\n",
    "                for e in missing_errors:\n",
    "                    logger.error(f\"FAILED - {e}\")\n",
    "                self.save_log(\"FAILED\", len(df), missing_errors)\n",
    "                return False\n",
    "\n",
    "            logger.info(\"PASSED - Check 2 Missing Values\")\n",
    "            logger.info(\"-\" * 40)\n",
    "\n",
    "            # ── check 3 ───────────────────────────────────\n",
    "            logger.info(\"CHECK 3 - VOLUME\")\n",
    "            volume_error = self.check_volume(df)\n",
    "\n",
    "            # returns string if problem, None if ok\n",
    "            if volume_error:\n",
    "                logger.error(f\"FAILED - {volume_error}\")\n",
    "                self.save_log(\"FAILED\", len(df), [volume_error])\n",
    "                return False\n",
    "\n",
    "            logger.info(f\"PASSED - Check 3 Volume - {len(df)} rows\")\n",
    "            logger.info(\"-\" * 40)\n",
    "\n",
    "            # ── check 4 ───────────────────────────────────\n",
    "            logger.info(\"CHECK 4 - CATEGORIES\")\n",
    "            category_error = self.check_categories(df)\n",
    "\n",
    "            # unknown category = warning not failure\n",
    "            if category_error:\n",
    "                logger.warning(f\"WARNING - {category_error}\")\n",
    "            else:\n",
    "                logger.info(\"PASSED - Check 4 Categories\")\n",
    "\n",
    "            logger.info(\"-\" * 40)\n",
    "            # ── check 5 ───────────────────────────────────\n",
    "            logger.info(\"CHECK 5 - RANGES\")\n",
    "            range_errors = self.check_ranges(df)\n",
    "\n",
    "            # any value outside logical bounds = failure\n",
    "            if range_errors:\n",
    "                for e in range_errors:\n",
    "                    logger.error(f\"FAILED - {e}\")\n",
    "                #self.save_log(\"FAILED\", len(df), range_errors)\n",
    "                return False\n",
    "\n",
    "            logger.info(\"PASSED - Check 5 Ranges\")\n",
    "            logger.info(\"-\" * 40)\n",
    "        \n",
    "\n",
    "\n",
    "            logger.info(\"PASSED - Check 5 Ranges\")\n",
    "            logger.info(\"-\" * 40)\n",
    "\n",
    "            # all checks passed\n",
    "            # now save the data\n",
    "            logger.info(\"Saving validated data...\")\n",
    "\n",
    "\n",
    "\n",
    "            timestamp   = datetime.now().strftime(\"%Y_%m_%d\")\n",
    "            weekly_path = os.path.join(\n",
    "            self.config.validated_data,\n",
    "            f\"validated_{timestamp}.csv\"\n",
    "        )\n",
    "            df.to_csv(weekly_path, index=False)\n",
    "            logger.info(f\"Weekly data saved: validated_{timestamp}.csv\")\n",
    "\n",
    "            master_path = os.path.join(self.config.validated_data,'Master_data.csv')\n",
    "\n",
    "            if os.path.exists(master_path):\n",
    "                existing_df = pd.read_csv(master_path)\n",
    "                combined_df = pd.concat(\n",
    "                    [existing_df, df],\n",
    "                    ignore_index=True\n",
    "                )\n",
    "\n",
    "                combined_df = combined_df.drop_duplicates(\n",
    "                    subset='asin',keep='last'\n",
    "                )\n",
    "                combined_df.to_csv(master_path, index=False)  # ← add this\n",
    "                logger.info(f\"Master updated: {len(combined_df)} rows\")\n",
    "                \n",
    "\n",
    "            else:\n",
    "                comdined_df = df\n",
    "                comdined_df.to_csv(master_path,index=False)\n",
    "                logger.info(f\"Master created: {len(combined_df)} rows\")\n",
    "\n",
    "            fallback_name = f\"fallback_{timestamp}.csv\"\n",
    "            fallback_path = os.path.join(\n",
    "                self.config.fallback_data,\n",
    "                fallback_name\n",
    "        )\n",
    "            combined_df.to_csv(fallback_path, index=False)\n",
    "            logger.info(f\"Fallback saved: {fallback_name}\")\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error: {e}\")\n",
    "\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ae6d7eee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2026-02-26 20:24:25,993: INFO: common: yaml file: config\\config.yaml loaded successfully]\n",
      "[2026-02-26 20:24:25,998: INFO: common: created directory at: artifacts]\n",
      "[2026-02-26 20:24:25,999: INFO: common: created directory at: artifacts/data_validation/validated/]\n",
      "[2026-02-26 20:24:26,001: INFO: common: created directory at: artifacts/data_validation/fallback/]\n",
      "[2026-02-26 20:24:26,001: INFO: common: created directory at: artifacts/data_validation/logs]\n",
      "[2026-02-26 20:24:26,006: INFO: 2170712202: ========================================]\n",
      "[2026-02-26 20:24:26,006: INFO: 2170712202: DATA VALIDATION STARTED]\n",
      "[2026-02-26 20:24:26,006: INFO: 2170712202: ========================================]\n",
      "[2026-02-26 20:24:26,139: INFO: 2170712202: Loaded: 16364 rows from artifacts/data_ingestion/raw_data/Amazon_Aesthetic_web_scrape.csv]\n",
      "[2026-02-26 20:24:26,144: INFO: 2170712202: ----------------------------------------]\n",
      "[2026-02-26 20:24:26,144: INFO: 2170712202: CHECK 1 - SCHEMA]\n",
      "[2026-02-26 20:24:26,144: INFO: 2170712202: PASSED - Check 1 Schema]\n",
      "[2026-02-26 20:24:26,144: INFO: 2170712202: ----------------------------------------]\n",
      "[2026-02-26 20:24:26,144: INFO: 2170712202: CHECK 2 - MISSING VALUES]\n",
      "[2026-02-26 20:24:26,163: INFO: 2170712202: PASSED - Check 2 Missing Values]\n",
      "[2026-02-26 20:24:26,163: INFO: 2170712202: ----------------------------------------]\n",
      "[2026-02-26 20:24:26,166: INFO: 2170712202: CHECK 3 - VOLUME]\n",
      "[2026-02-26 20:24:26,166: INFO: 2170712202: PASSED - Check 3 Volume - 16364 rows]\n",
      "[2026-02-26 20:24:26,166: INFO: 2170712202: ----------------------------------------]\n",
      "[2026-02-26 20:24:26,166: INFO: 2170712202: CHECK 4 - CATEGORIES]\n",
      "[2026-02-26 20:24:26,174: INFO: 2170712202: PASSED - Check 4 Categories]\n",
      "[2026-02-26 20:24:26,174: INFO: 2170712202: ----------------------------------------]\n",
      "[2026-02-26 20:24:26,177: INFO: 2170712202: CHECK 5 - RANGES]\n",
      "[2026-02-26 20:24:26,183: INFO: 2170712202: PASSED - Check 5 Ranges]\n",
      "[2026-02-26 20:24:26,186: INFO: 2170712202: ----------------------------------------]\n",
      "[2026-02-26 20:24:26,186: INFO: 2170712202: PASSED - Check 5 Ranges]\n",
      "[2026-02-26 20:24:26,188: INFO: 2170712202: ----------------------------------------]\n",
      "[2026-02-26 20:24:26,188: INFO: 2170712202: Saving validated data...]\n",
      "[2026-02-26 20:24:26,342: INFO: 2170712202: Weekly data saved: validated_2026_02_26.csv]\n",
      "[2026-02-26 20:24:26,545: INFO: 2170712202: Master updated: 13813 rows]\n",
      "[2026-02-26 20:24:26,666: INFO: 2170712202: Fallback saved: fallback_2026_02_26.csv]\n"
     ]
    }
   ],
   "source": [
    "con = Config_manager()\n",
    "data_validation = con.get_data_validation_config()\n",
    "data_validation = Data_validation_check(data_validation)\n",
    "#df = pd.read_csv(r\"C:\\Users\\sam\\End-to-End-Fashion-Recommendation-System-with-MLOps\\Data_set\\Amazon_Aesthetic_web_scrape.csv\")\n",
    "errors = data_validation.validate_data()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0daa9ffe",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2a2c4d1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa961894",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
